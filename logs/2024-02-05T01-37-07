02/05/2024 01:37:07 - INFO - __main__ - Namespace(do_tensorize=False, tensorize_dir='tensorized', n_gpu=1, n_process=40, n_prefix_tokens=10, use_demonstrations=False, log_dir='logs', prefix_embed_file=None, dataset=None, task='tune', split='train', data_dir='data/', k=16384, test_k=4, seed=100, train_seed=1, lr=1e-05, warmup_steps=0, batch_size=8, num_training_steps=5000, weight_decay=0.0, no_masking=False, use_random_english_words=False, out_dir='checkpoints\\gpt2\\tune-train\\prefix={10}-{channel}-lr={1e-5}-initByVocab', method='channel', gpt2='gpt2', optimization='adamw', fp16=False, local_rank=-1)
02/05/2024 01:37:07 - INFO - __main__ - batch_size=8	max_length=256	max_length_per_example=256
02/05/2024 01:37:07 - INFO - __main__ - [Train] glue-sst2	16384
02/05/2024 01:37:07 - INFO - __main__ - [Train] amazon_polarity	10000
02/05/2024 01:37:07 - INFO - __main__ - [Train] financial_phrasebank	1811
02/05/2024 01:37:07 - INFO - __main__ - [Train] poem_sentiment	843
02/05/2024 01:37:07 - INFO - __main__ - [Train] yelp_polarity	16384
02/05/2024 01:37:07 - INFO - __main__ - [Train] glue-cola	8551
02/05/2024 01:37:07 - INFO - __main__ - [Train] blimp-sentential_negation_npi_scope	800
02/05/2024 01:37:07 - INFO - __main__ - [Train] blimp-sentential_negation_npi_licensor_present	800
02/05/2024 01:37:07 - INFO - __main__ - [Train] blimp-ellipsis_n_bar_2	800
02/05/2024 01:37:07 - INFO - __main__ - [Train] blimp-anaphor_number_agreement	800
02/05/2024 01:37:07 - INFO - __main__ - [Train] ag_news	16384
02/05/2024 01:37:07 - INFO - __main__ - [Train] dbpedia_14	16384
02/05/2024 01:37:07 - INFO - __main__ - [Train] ethos-sexual_orientation	346
02/05/2024 01:37:07 - INFO - __main__ - [Train] ethos-religion	346
02/05/2024 01:37:07 - INFO - __main__ - [Train] ethos-race	346
02/05/2024 01:37:07 - INFO - __main__ - [Train] ethos-gender	346
02/05/2024 01:37:07 - INFO - __main__ - [Train] ethos-disability	346
02/05/2024 01:37:07 - INFO - __main__ - [Train] ethos-directed_vs_generalized	346
02/05/2024 01:37:07 - INFO - __main__ - [Train] emo	16384
02/05/2024 01:37:07 - INFO - __main__ - [Train] emotion	16000
02/05/2024 01:37:07 - INFO - __main__ - channel on None (20 train)
02/05/2024 01:37:10 - INFO - __main__ - tensorized\tune_channel_k=124401_seed=100_length=10-256-rank=%d.pkl
02/05/2024 01:37:13 - INFO - __main__ - Checking the first example...
Input:
<ag_news-0><ag_news-1><ag_news-2><ag_news-3><ag_news-4><ag_news-5><ag_news-6><ag_news-7><ag_news-8><ag_news-9>Sports
Output:
 Nuggets hang on to Nene DENVER The Denver Nuggets have picked up their three million dollar option for next NBA season for forward Nene (nuh-NAY #39;) but not for Nikoloz Tskitishvili (NIH-koh-lohs skee-tish-VEE #39;-lee).
02/05/2024 01:37:13 - INFO - __main__ - checkpoints\gpt2\tune-train\prefix={10}-{channel}-lr={1e-5}-initByVocab
02/05/2024 01:37:13 - INFO - __main__ - Setting up for local_rank=-1, world_size=1
02/05/2024 01:37:19 - INFO - __main__ - torch.Size([124401, 256])
02/05/2024 01:37:19 - INFO - __main__ - Training 1 parameters on 124401 examples for 5000 steps using 1 GPUs
02/05/2024 01:38:01 - INFO - __main__ - local rank -1	global step 100	train loss 9.30
02/05/2024 01:38:44 - INFO - __main__ - local rank -1	global step 200	train loss 9.55
02/05/2024 01:39:27 - INFO - __main__ - local rank -1	global step 300	train loss 9.49
02/05/2024 01:40:10 - INFO - __main__ - local rank -1	global step 400	train loss 9.32
02/05/2024 01:40:53 - INFO - __main__ - local rank -1	global step 500	train loss 9.11
02/05/2024 01:41:36 - INFO - __main__ - local rank -1	global step 600	train loss 9.00
02/05/2024 01:42:19 - INFO - __main__ - local rank -1	global step 700	train loss 8.86
02/05/2024 01:43:02 - INFO - __main__ - local rank -1	global step 800	train loss 8.86
02/05/2024 01:43:45 - INFO - __main__ - local rank -1	global step 900	train loss 8.73
02/05/2024 01:44:28 - INFO - __main__ - local rank -1	global step 1000	train loss 8.57
02/05/2024 01:45:11 - INFO - __main__ - local rank -1	global step 1100	train loss 8.65
02/05/2024 01:45:54 - INFO - __main__ - local rank -1	global step 1200	train loss 8.54
02/05/2024 01:46:37 - INFO - __main__ - local rank -1	global step 1300	train loss 8.65
02/05/2024 01:47:20 - INFO - __main__ - local rank -1	global step 1400	train loss 8.52
02/05/2024 01:48:02 - INFO - __main__ - local rank -1	global step 1500	train loss 8.53
02/05/2024 01:48:45 - INFO - __main__ - local rank -1	global step 1600	train loss 8.44
02/05/2024 01:49:28 - INFO - __main__ - local rank -1	global step 1700	train loss 8.44
02/05/2024 01:50:11 - INFO - __main__ - local rank -1	global step 1800	train loss 8.45
02/05/2024 01:50:54 - INFO - __main__ - local rank -1	global step 1900	train loss 8.39
02/05/2024 01:51:37 - INFO - __main__ - local rank -1	global step 2000	train loss 8.31
02/05/2024 01:52:20 - INFO - __main__ - local rank -1	global step 2100	train loss 8.28
02/05/2024 01:53:03 - INFO - __main__ - local rank -1	global step 2200	train loss 8.31
02/05/2024 01:53:46 - INFO - __main__ - local rank -1	global step 2300	train loss 8.27
02/05/2024 01:54:29 - INFO - __main__ - local rank -1	global step 2400	train loss 8.29
02/05/2024 01:55:12 - INFO - __main__ - local rank -1	global step 2500	train loss 8.29
02/05/2024 01:55:57 - INFO - __main__ - local rank -1	global step 2600	train loss 8.31
02/05/2024 01:56:41 - INFO - __main__ - local rank -1	global step 2700	train loss 8.25
02/05/2024 01:57:25 - INFO - __main__ - local rank -1	global step 2800	train loss 8.33
02/05/2024 01:58:09 - INFO - __main__ - local rank -1	global step 2900	train loss 8.18
02/05/2024 01:58:52 - INFO - __main__ - local rank -1	global step 3000	train loss 8.15
02/05/2024 01:59:35 - INFO - __main__ - local rank -1	global step 3100	train loss 8.15
02/05/2024 02:00:18 - INFO - __main__ - local rank -1	global step 3200	train loss 8.24
02/05/2024 02:01:01 - INFO - __main__ - local rank -1	global step 3300	train loss 8.17
02/05/2024 02:01:44 - INFO - __main__ - local rank -1	global step 3400	train loss 8.22
02/05/2024 02:02:27 - INFO - __main__ - local rank -1	global step 3500	train loss 8.19
02/05/2024 02:03:10 - INFO - __main__ - local rank -1	global step 3600	train loss 8.19
02/05/2024 02:03:53 - INFO - __main__ - local rank -1	global step 3700	train loss 8.17
02/05/2024 02:04:36 - INFO - __main__ - local rank -1	global step 3800	train loss 8.11
02/05/2024 02:05:19 - INFO - __main__ - local rank -1	global step 3900	train loss 8.05
02/05/2024 02:06:02 - INFO - __main__ - local rank -1	global step 4000	train loss 8.14
02/05/2024 02:06:45 - INFO - __main__ - local rank -1	global step 4100	train loss 8.20
02/05/2024 02:07:28 - INFO - __main__ - local rank -1	global step 4200	train loss 8.14
02/05/2024 02:08:11 - INFO - __main__ - local rank -1	global step 4300	train loss 8.03
02/05/2024 02:08:54 - INFO - __main__ - local rank -1	global step 4400	train loss 8.12
02/05/2024 02:09:37 - INFO - __main__ - local rank -1	global step 4500	train loss 8.14
02/05/2024 02:10:20 - INFO - __main__ - local rank -1	global step 4600	train loss 8.13
02/05/2024 02:11:03 - INFO - __main__ - local rank -1	global step 4700	train loss 8.15
02/05/2024 02:11:46 - INFO - __main__ - local rank -1	global step 4800	train loss 8.14
02/05/2024 02:12:29 - INFO - __main__ - local rank -1	global step 4900	train loss 8.17
02/05/2024 02:13:13 - INFO - __main__ - local rank -1	global step 5000	train loss 8.09
02/05/2024 02:13:13 - INFO - __main__ - Finish training

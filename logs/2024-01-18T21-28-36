01/18/2024 21:28:36 - INFO - __main__ - Namespace(do_tensorize=False, tensorize_dir='tensorized', n_gpu=1, n_process=40, n_prefix_tokens=10, use_demonstrations=False, log_dir='logs', prefix_embed_file=None, dataset=None, task='glue', split='glue', data_dir='data/', k=16384, test_k=4, seed=100, train_seed=1, lr=1e-05, warmup_steps=0, batch_size=8, num_training_steps=3000, weight_decay=0.0, no_masking=False, use_random_english_words=False, out_dir='checkpoints\\gpt2-medium\\glue-glue\\prefix={10}-{channel}-lr={1e-5}-initByVocab', method='channel', gpt2='gpt2-medium', optimization='adamw', fp16=False, local_rank=-1)
01/18/2024 21:28:36 - INFO - __main__ - batch_size=8	max_length=256	max_length_per_example=256
01/18/2024 21:28:36 - INFO - __main__ - [Train] glue-cola	8551
01/18/2024 21:28:36 - INFO - __main__ - [Train] glue-mnli	16384
01/18/2024 21:28:36 - INFO - __main__ - [Train] glue-qqp	16384
01/18/2024 21:28:36 - INFO - __main__ - [Train] glue-mrpc	3668
01/18/2024 21:28:36 - INFO - __main__ - [Train] glue-qnli	16384
01/18/2024 21:28:36 - INFO - __main__ - [Train] glue-rte	2490
01/18/2024 21:28:36 - INFO - __main__ - [Train] glue-sst2	16384
01/18/2024 21:28:36 - INFO - __main__ - [Train] glue-wnli	635
01/18/2024 21:28:36 - INFO - __main__ - channel on None (8 train)
01/18/2024 21:28:39 - INFO - __main__ - tensorized\glue_channel_k=80880_seed=100_length=10-256-rank=%d.pkl
01/18/2024 21:28:41 - INFO - __main__ - Checking the first example...
Input:
<glue-qnli-0><glue-qnli-1><glue-qnli-2><glue-qnli-3><glue-qnli-4><glue-qnli-5><glue-qnli-6><glue-qnli-7><glue-qnli-8><glue-qnli-9>entailment
Output:
 question: What is Yale largely known for? [SEP] sentence: Yale is noted for its largely Collegiate Gothic campus as well as for several iconic modern buildings commonly discussed in architectural history survey courses: Louis Kahn's Yale Art Gallery and Center for British Art, Eero Saarinen's Ingalls Rink and Ezra Stiles and Morse Colleges, and Paul Rudolph's Art & Architecture Building.
01/18/2024 21:28:41 - INFO - __main__ - checkpoints\gpt2-medium\glue-glue\prefix={10}-{channel}-lr={1e-5}-initByVocab
01/18/2024 21:28:41 - INFO - __main__ - Setting up for local_rank=-1, world_size=1
01/18/2024 21:28:47 - INFO - __main__ - torch.Size([80880, 256])
01/18/2024 21:28:47 - INFO - __main__ - Training 1 parameters on 80880 examples for 3000 steps using 1 GPUs
01/18/2024 21:34:43 - INFO - __main__ - local rank -1	global step 100	train loss 11.82
01/18/2024 21:40:42 - INFO - __main__ - local rank -1	global step 200	train loss 11.11
01/18/2024 21:46:36 - INFO - __main__ - local rank -1	global step 300	train loss 10.41
01/18/2024 21:52:31 - INFO - __main__ - local rank -1	global step 400	train loss 9.91
01/18/2024 21:58:25 - INFO - __main__ - local rank -1	global step 500	train loss 9.57
01/18/2024 22:04:20 - INFO - __main__ - local rank -1	global step 600	train loss 9.30
01/18/2024 22:10:14 - INFO - __main__ - local rank -1	global step 700	train loss 9.22
01/18/2024 22:16:09 - INFO - __main__ - local rank -1	global step 800	train loss 9.06
01/18/2024 22:22:04 - INFO - __main__ - local rank -1	global step 900	train loss 9.01
01/18/2024 22:28:00 - INFO - __main__ - local rank -1	global step 1000	train loss 8.93
01/18/2024 22:33:57 - INFO - __main__ - local rank -1	global step 1100	train loss 8.92
01/18/2024 22:39:52 - INFO - __main__ - local rank -1	global step 1200	train loss 8.95
01/18/2024 22:45:48 - INFO - __main__ - local rank -1	global step 1300	train loss 8.86
01/18/2024 22:51:43 - INFO - __main__ - local rank -1	global step 1400	train loss 8.89
01/18/2024 22:57:38 - INFO - __main__ - local rank -1	global step 1500	train loss 8.77
01/18/2024 23:03:33 - INFO - __main__ - local rank -1	global step 1600	train loss 8.76
01/18/2024 23:09:30 - INFO - __main__ - local rank -1	global step 1700	train loss 8.72
01/18/2024 23:15:24 - INFO - __main__ - local rank -1	global step 1800	train loss 8.78
01/18/2024 23:21:19 - INFO - __main__ - local rank -1	global step 1900	train loss 8.72
01/18/2024 23:27:14 - INFO - __main__ - local rank -1	global step 2000	train loss 8.68
01/18/2024 23:33:08 - INFO - __main__ - local rank -1	global step 2100	train loss 8.73
01/18/2024 23:39:02 - INFO - __main__ - local rank -1	global step 2200	train loss 8.69
01/18/2024 23:44:57 - INFO - __main__ - local rank -1	global step 2300	train loss 8.72
01/18/2024 23:50:55 - INFO - __main__ - local rank -1	global step 2400	train loss 8.73
01/18/2024 23:56:49 - INFO - __main__ - local rank -1	global step 2500	train loss 8.73
01/19/2024 00:02:44 - INFO - __main__ - local rank -1	global step 2600	train loss 8.67
01/19/2024 00:08:42 - INFO - __main__ - local rank -1	global step 2700	train loss 8.57
01/19/2024 00:14:39 - INFO - __main__ - local rank -1	global step 2800	train loss 8.66
01/19/2024 00:20:33 - INFO - __main__ - local rank -1	global step 2900	train loss 8.68
01/19/2024 00:26:27 - INFO - __main__ - local rank -1	global step 3000	train loss 8.72
01/19/2024 00:26:30 - INFO - __main__ - Finish training

01/13/2024 16:19:15 - INFO - __main__ - Namespace(do_tensorize=False, tensorize_dir='tensorized', n_gpu=1, n_process=40, n_prefix_tokens=10, use_demonstrations=False, log_dir='logs', prefix_embed_file=None, dataset=None, task='glue', split='glue', data_dir='data/', k=16384, test_k=4, seed=100, train_seed=1, lr=0.01, warmup_steps=0, batch_size=16, num_training_steps=3000, weight_decay=0.0, no_masking=False, use_random_english_words=False, out_dir='checkpoints\\gpt2\\glue-glue\\prefix={10}-{direct}-lr={1e-2}-initByVocab', method='direct', gpt2='gpt2', optimization='adamw', fp16=False, local_rank=-1)
01/13/2024 16:19:15 - INFO - __main__ - batch_size=16	max_length=256	max_length_per_example=256
01/13/2024 16:19:15 - INFO - __main__ - [Train] glue-cola	8551
01/13/2024 16:19:15 - INFO - __main__ - [Train] glue-mnli	16384
01/13/2024 16:19:15 - INFO - __main__ - [Train] glue-qqp	16384
01/13/2024 16:19:15 - INFO - __main__ - [Train] glue-mrpc	3668
01/13/2024 16:19:15 - INFO - __main__ - [Train] glue-qnli	16384
01/13/2024 16:19:15 - INFO - __main__ - [Train] glue-rte	2490
01/13/2024 16:19:15 - INFO - __main__ - [Train] glue-sst2	16384
01/13/2024 16:19:15 - INFO - __main__ - [Train] glue-wnli	635
01/13/2024 16:19:15 - INFO - __main__ - direct on None (8 train)
01/13/2024 16:19:18 - INFO - __main__ - tensorized\glue_direct_k=80880_seed=100_length=10-256-rank=%d.pkl
01/13/2024 16:19:21 - INFO - __main__ - Checking the first example...
Input:
<glue-mrpc-0><glue-mrpc-1><glue-mrpc-2><glue-mrpc-3><glue-mrpc-4><glue-mrpc-5><glue-mrpc-6><glue-mrpc-7><glue-mrpc-8><glue-mrpc-9>sentence 1: Officials said one provision in the Senate-passed measure accounted for $ 40 billion over 10 years in the CBO calculation. [SEP] sentence 2: The budget office said one provision of the Senate bill accounted for $ 40 billion of the cost.
Output:
 equivalent
01/13/2024 16:19:21 - INFO - __main__ - checkpoints\gpt2\glue-glue\prefix={10}-{direct}-lr={1e-2}-initByVocab
01/13/2024 16:19:21 - INFO - __main__ - Setting up for local_rank=-1, world_size=1
01/13/2024 16:19:25 - INFO - __main__ - torch.Size([80880, 256])
01/13/2024 16:19:25 - INFO - __main__ - Training 1 parameters on 80880 examples for 3000 steps using 1 GPUs
01/13/2024 16:20:58 - INFO - __main__ - local rank -1	global step 100	train loss 2.36
01/13/2024 16:22:31 - INFO - __main__ - local rank -1	global step 200	train loss 1.94
01/13/2024 16:24:04 - INFO - __main__ - local rank -1	global step 300	train loss 1.71
01/13/2024 16:25:37 - INFO - __main__ - local rank -1	global step 400	train loss 1.59
01/13/2024 16:27:10 - INFO - __main__ - local rank -1	global step 500	train loss 1.40
01/13/2024 16:28:44 - INFO - __main__ - local rank -1	global step 600	train loss 1.61
01/13/2024 16:30:17 - INFO - __main__ - local rank -1	global step 700	train loss 1.38
01/13/2024 16:31:50 - INFO - __main__ - local rank -1	global step 800	train loss 1.48
01/13/2024 16:33:23 - INFO - __main__ - local rank -1	global step 900	train loss 1.38
01/13/2024 16:34:56 - INFO - __main__ - local rank -1	global step 1000	train loss 1.47
01/13/2024 16:36:29 - INFO - __main__ - local rank -1	global step 1100	train loss 1.37
01/13/2024 16:38:02 - INFO - __main__ - local rank -1	global step 1200	train loss 1.17
01/13/2024 16:39:35 - INFO - __main__ - local rank -1	global step 1300	train loss 1.32
01/13/2024 16:41:08 - INFO - __main__ - local rank -1	global step 1400	train loss 1.30
01/13/2024 16:42:41 - INFO - __main__ - local rank -1	global step 1500	train loss 1.13
01/13/2024 16:44:14 - INFO - __main__ - local rank -1	global step 1600	train loss 1.16
01/13/2024 16:45:47 - INFO - __main__ - local rank -1	global step 1700	train loss 1.19
01/13/2024 16:47:21 - INFO - __main__ - local rank -1	global step 1800	train loss 1.16
01/13/2024 16:48:54 - INFO - __main__ - local rank -1	global step 1900	train loss 1.12
01/13/2024 16:50:27 - INFO - __main__ - local rank -1	global step 2000	train loss 0.97
01/13/2024 16:52:00 - INFO - __main__ - local rank -1	global step 2100	train loss 1.05
01/13/2024 16:53:33 - INFO - __main__ - local rank -1	global step 2200	train loss 0.93
01/13/2024 16:55:06 - INFO - __main__ - local rank -1	global step 2300	train loss 0.93
01/13/2024 16:56:40 - INFO - __main__ - local rank -1	global step 2400	train loss 0.83
01/13/2024 16:58:13 - INFO - __main__ - local rank -1	global step 2500	train loss 0.89
01/13/2024 16:59:46 - INFO - __main__ - local rank -1	global step 2600	train loss 0.87
01/13/2024 17:01:19 - INFO - __main__ - local rank -1	global step 2700	train loss 0.81
01/13/2024 17:02:52 - INFO - __main__ - local rank -1	global step 2800	train loss 0.80
01/13/2024 17:04:25 - INFO - __main__ - local rank -1	global step 2900	train loss 0.73
01/13/2024 17:05:58 - INFO - __main__ - local rank -1	global step 3000	train loss 0.76
01/13/2024 17:05:58 - INFO - __main__ - Finish training

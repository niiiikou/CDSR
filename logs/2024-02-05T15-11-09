02/05/2024 15:11:09 - INFO - __main__ - Namespace(do_tensorize=False, tensorize_dir='tensorized', n_gpu=1, n_process=40, n_prefix_tokens=10, use_demonstrations=False, log_dir='logs', prefix_embed_file=None, dataset=None, task='tune', split='train', data_dir='data/', k=16384, test_k=4, seed=100, train_seed=1, lr=0.001, warmup_steps=0, batch_size=8, num_training_steps=5000, weight_decay=0.0, no_masking=False, use_random_english_words=False, out_dir='checkpoints\\gpt2\\tune-train\\prefix={10}-{channel}-lr={1e-3}-initByVocab', method='channel', gpt2='gpt2', optimization='adamw', fp16=False, local_rank=-1)
02/05/2024 15:11:09 - INFO - __main__ - batch_size=8	max_length=256	max_length_per_example=256
02/05/2024 15:11:10 - INFO - __main__ - [Train] glue-sst2	16384
02/05/2024 15:11:10 - INFO - __main__ - [Train] amazon_polarity	10000
02/05/2024 15:11:10 - INFO - __main__ - [Train] financial_phrasebank	1811
02/05/2024 15:11:10 - INFO - __main__ - [Train] poem_sentiment	843
02/05/2024 15:11:10 - INFO - __main__ - [Train] yelp_polarity	16384
02/05/2024 15:11:10 - INFO - __main__ - [Train] glue-cola	8551
02/05/2024 15:11:10 - INFO - __main__ - [Train] blimp-sentential_negation_npi_scope	800
02/05/2024 15:11:10 - INFO - __main__ - [Train] blimp-sentential_negation_npi_licensor_present	800
02/05/2024 15:11:10 - INFO - __main__ - [Train] blimp-ellipsis_n_bar_2	800
02/05/2024 15:11:10 - INFO - __main__ - [Train] blimp-anaphor_number_agreement	800
02/05/2024 15:11:10 - INFO - __main__ - [Train] ag_news	16384
02/05/2024 15:11:10 - INFO - __main__ - [Train] dbpedia_14	16384
02/05/2024 15:11:10 - INFO - __main__ - [Train] ethos-sexual_orientation	346
02/05/2024 15:11:10 - INFO - __main__ - [Train] ethos-religion	346
02/05/2024 15:11:10 - INFO - __main__ - [Train] ethos-race	346
02/05/2024 15:11:10 - INFO - __main__ - [Train] ethos-gender	346
02/05/2024 15:11:10 - INFO - __main__ - [Train] ethos-disability	346
02/05/2024 15:11:10 - INFO - __main__ - [Train] ethos-directed_vs_generalized	346
02/05/2024 15:11:10 - INFO - __main__ - [Train] emo	16384
02/05/2024 15:11:10 - INFO - __main__ - [Train] emotion	16000
02/05/2024 15:11:10 - INFO - __main__ - channel on None (20 train)
02/05/2024 15:11:18 - INFO - __main__ - tensorized\tune_channel_k=124401_seed=100_length=10-256-rank=%d.pkl
02/05/2024 15:11:21 - INFO - __main__ - Checking the first example...
Input:
<ag_news-0><ag_news-1><ag_news-2><ag_news-3><ag_news-4><ag_news-5><ag_news-6><ag_news-7><ag_news-8><ag_news-9>Sports
Output:
 Nuggets hang on to Nene DENVER The Denver Nuggets have picked up their three million dollar option for next NBA season for forward Nene (nuh-NAY #39;) but not for Nikoloz Tskitishvili (NIH-koh-lohs skee-tish-VEE #39;-lee).
02/05/2024 15:11:21 - INFO - __main__ - checkpoints\gpt2\tune-train\prefix={10}-{channel}-lr={1e-3}-initByVocab
02/05/2024 15:11:21 - INFO - __main__ - Setting up for local_rank=-1, world_size=1
02/05/2024 15:11:28 - INFO - __main__ - torch.Size([124401, 256])
02/05/2024 15:11:28 - INFO - __main__ - Training 1 parameters on 124401 examples for 5000 steps using 1 GPUs
02/05/2024 15:12:11 - INFO - __main__ - local rank -1	global step 100	train loss 8.17
02/05/2024 15:12:56 - INFO - __main__ - local rank -1	global step 200	train loss 8.15
02/05/2024 15:13:42 - INFO - __main__ - local rank -1	global step 300	train loss 8.06
02/05/2024 15:14:28 - INFO - __main__ - local rank -1	global step 400	train loss 7.97
02/05/2024 15:15:11 - INFO - __main__ - local rank -1	global step 500	train loss 7.90
02/05/2024 15:15:55 - INFO - __main__ - local rank -1	global step 600	train loss 7.85
02/05/2024 15:16:38 - INFO - __main__ - local rank -1	global step 700	train loss 7.74
02/05/2024 15:17:22 - INFO - __main__ - local rank -1	global step 800	train loss 7.66
02/05/2024 15:18:08 - INFO - __main__ - local rank -1	global step 900	train loss 7.61
02/05/2024 15:18:51 - INFO - __main__ - local rank -1	global step 1000	train loss 7.53
02/05/2024 15:19:36 - INFO - __main__ - local rank -1	global step 1100	train loss 7.61
02/05/2024 15:20:22 - INFO - __main__ - local rank -1	global step 1200	train loss 7.49
02/05/2024 15:21:06 - INFO - __main__ - local rank -1	global step 1300	train loss 7.47
02/05/2024 15:21:50 - INFO - __main__ - local rank -1	global step 1400	train loss 7.45
02/05/2024 15:22:38 - INFO - __main__ - local rank -1	global step 1500	train loss 7.52
02/05/2024 15:23:26 - INFO - __main__ - local rank -1	global step 1600	train loss 7.41
02/05/2024 15:24:13 - INFO - __main__ - local rank -1	global step 1700	train loss 7.27
02/05/2024 15:24:59 - INFO - __main__ - local rank -1	global step 1800	train loss 7.37
02/05/2024 15:25:46 - INFO - __main__ - local rank -1	global step 1900	train loss 7.37
02/05/2024 15:26:33 - INFO - __main__ - local rank -1	global step 2000	train loss 7.29
02/05/2024 15:27:23 - INFO - __main__ - local rank -1	global step 2100	train loss 7.29
02/05/2024 15:28:14 - INFO - __main__ - local rank -1	global step 2200	train loss 7.18
02/05/2024 15:29:01 - INFO - __main__ - local rank -1	global step 2300	train loss 7.21
02/05/2024 15:29:47 - INFO - __main__ - local rank -1	global step 2400	train loss 7.17
02/05/2024 15:30:33 - INFO - __main__ - local rank -1	global step 2500	train loss 7.23
02/05/2024 15:31:19 - INFO - __main__ - local rank -1	global step 2600	train loss 7.14
02/05/2024 15:32:05 - INFO - __main__ - local rank -1	global step 2700	train loss 7.15
02/05/2024 15:32:50 - INFO - __main__ - local rank -1	global step 2800	train loss 7.13
02/05/2024 15:33:36 - INFO - __main__ - local rank -1	global step 2900	train loss 7.03
02/05/2024 15:34:22 - INFO - __main__ - local rank -1	global step 3000	train loss 7.08
02/05/2024 15:35:06 - INFO - __main__ - local rank -1	global step 3100	train loss 7.00
02/05/2024 15:35:49 - INFO - __main__ - local rank -1	global step 3200	train loss 7.02
02/05/2024 15:36:32 - INFO - __main__ - local rank -1	global step 3300	train loss 7.03
02/05/2024 15:37:15 - INFO - __main__ - local rank -1	global step 3400	train loss 7.08
02/05/2024 15:37:58 - INFO - __main__ - local rank -1	global step 3500	train loss 6.95
02/05/2024 15:38:41 - INFO - __main__ - local rank -1	global step 3600	train loss 7.01
02/05/2024 15:39:26 - INFO - __main__ - local rank -1	global step 3700	train loss 7.03
02/05/2024 15:40:12 - INFO - __main__ - local rank -1	global step 3800	train loss 6.96
02/05/2024 15:40:57 - INFO - __main__ - local rank -1	global step 3900	train loss 6.99
02/05/2024 15:41:42 - INFO - __main__ - local rank -1	global step 4000	train loss 7.02
02/05/2024 15:42:29 - INFO - __main__ - local rank -1	global step 4100	train loss 6.93
02/05/2024 15:43:15 - INFO - __main__ - local rank -1	global step 4200	train loss 6.93
02/05/2024 15:44:04 - INFO - __main__ - local rank -1	global step 4300	train loss 6.90
02/05/2024 15:44:51 - INFO - __main__ - local rank -1	global step 4400	train loss 6.98
02/05/2024 15:45:38 - INFO - __main__ - local rank -1	global step 4500	train loss 6.91
02/05/2024 15:46:26 - INFO - __main__ - local rank -1	global step 4600	train loss 6.97
02/05/2024 15:47:13 - INFO - __main__ - local rank -1	global step 4700	train loss 6.91
02/05/2024 15:47:57 - INFO - __main__ - local rank -1	global step 4800	train loss 6.91
02/05/2024 15:48:43 - INFO - __main__ - local rank -1	global step 4900	train loss 6.93
02/05/2024 15:49:28 - INFO - __main__ - local rank -1	global step 5000	train loss 6.92
02/05/2024 15:49:29 - INFO - __main__ - Finish training
